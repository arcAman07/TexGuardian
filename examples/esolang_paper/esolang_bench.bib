@article{chen2021humaneval,
  title={Evaluating Large Language Models Trained on Code},
  author={Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde de Oliveira and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and others},
  journal={arXiv preprint arXiv:2107.03374},
  year={2021}
}

@inproceedings{min2022icl,
  title={Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?},
  author={Min, Sewon and Lyu, Xinxi and Holtzman, Ari and Artetxe, Mikel and Lewis, Mike and Hajishirzi, Hannaneh and Zettlemoyer, Luke},
  booktitle={Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
  pages={11048--11064},
  year={2022}
}

@article{liu2024lost,
  title={Lost in the Middle: How Language Models Use Long Contexts},
  author={Liu, Nelson F and Lin, Kevin and Hewitt, John and Paranjape, Ashwin and Bevilacqua, Michele and Petroni, Fabio and Liang, Percy},
  journal={Transactions of the Association for Computational Linguistics},
  volume={12},
  pages={157--173},
  year={2024}
}

@article{austin2021mbpp,
  title={Program Synthesis with Large Language Models},
  author={Austin, Jacob and Odena, Augustus and Nye, Maxwell and Bosma, Maarten and Michalewski, Henryk and Dohan, David and Jiang, Ellen and Cai, Carrie and Terry, Michael and Le, Quoc and others},
  journal={arXiv preprint arXiv:2108.07732},
  year={2021}
}

@article{zhang2024gsm1k,
  title={A Careful Examination of Large Language Model Performance on Grade School Arithmetic},
  author={Zhang, Hugh and Da, Jeff and Lee, Dean and Robinson, Vaughn and Wu, Catherine and Song, Will and Zhao, Tiffany and Raja, Pranav and Zhuang, Charlotte and Slack, Dylan and Lyu, Qin and Hendryx, Sean and Kaplan, Russell and Lunati, Michele and Yue, Summer},
  journal={arXiv preprint arXiv:2405.00332},
  year={2024}
}

@inproceedings{sainz2023contamination,
  title={{NLP} Evaluation in Trouble: On the Need to Measure {LLM} Data Contamination for each Benchmark},
  author={Sainz, Oscar and Campos, Jon and Garc{\'\i}a-Ferrero, Iker and Etxaniz, Julen and Lopez de Lacalle, Oier and Agirre, Eneko},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2023},
  pages={10776--10787},
  year={2023}
}

@article{gupta2024benchmark,
  title={Changing Answer Order Can Decrease {MMLU} Accuracy},
  author={Gupta, Vipul and Pantoja, David and Ross, Candace and Williams, Adina and Ung, Megan},
  journal={arXiv preprint arXiv:2406.19470},
  year={2024}
}

@article{jimenez2024swebench,
  title={{SWE-bench}: Can Language Models Resolve Real-World {GitHub} Issues?},
  author={Jimenez, Carlos E and Yang, John and Wettig, Alexander and Yao, Shunyu and Pei, Kexin and Press, Ofir and Narasimhan, Karthik},
  journal={arXiv preprint arXiv:2310.06770},
  year={2024}
}

@article{cassano2023multiple,
  title={{MultiPL-E}: A Scalable and Polyglot Approach to Benchmarking Neural Code Generation},
  author={Cassano, Federico and Gouwar, John and Lucchetti, Daniel and Schlesinger, Claire and Freeman, Dennis and Anderson, Carolyn Jane and Feldman, Molly Q and Greenberg, Michael and Jangda, Abhinav and Guha, Arjun},
  journal={IEEE Transactions on Software Engineering},
  year={2023}
}

@inproceedings{bendavid2010domain,
  title={A Theory of Learning from Different Domains},
  author={Ben-David, Shai and Blitzer, John and Crammer, Koby and Kulesza, Alex and Pereira, Fernando and Vaughan, Jennifer Wortman},
  booktitle={Machine Learning},
  volume={79},
  pages={151--175},
  year={2010}
}

@article{dziri2024compositional,
  title={Faith and Fate: Limits of Transformers on Compositionality},
  author={Dziri, Nouha and Lu, Ximing and Sclar, Melanie and Li, Xiang Lorraine and Jiang, Liwei and Lin, Bill Yuchen and West, Peter and Bhagavatula, Chandra and Bras, Ronan Le and Hwang, Jena D and others},
  journal={Advances in Neural Information Processing Systems},
  year={2024}
}

@article{wei2022cot,
  title={Chain-of-Thought Prompting Elicits Reasoning in Large Language Models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={24824--24837},
  year={2022}
}

@article{chen2023selfdebugging,
  title={Teaching Large Language Models to Self-Debug},
  author={Chen, Xinyun and Lin, Maxwell and Sch{\"a}rli, Nathanael and Zhou, Denny},
  journal={arXiv preprint arXiv:2304.05128},
  year={2023}
}

@article{yao2023react,
  title={{ReAct}: Synergizing Reasoning and Acting in Language Models},
  author={Yao, Shunyu and Zhao, Jeffrey and Yu, Dian and Du, Nan and Shafran, Izhak and Narasimhan, Karthik and Cao, Yuan},
  journal={arXiv preprint arXiv:2210.03629},
  year={2023}
}

@article{brown2020gpt3,
  title={Language Models are Few-Shot Learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@inproceedings{jacovi2023contamination,
  title={Stop Uploading Test Data in Plain Text: Practical Strategies for Mitigating Data Contamination by Evaluation Benchmarks},
  author={Jacovi, Alon and Caciularu, Avi and Goldman, Omer and Goldberg, Yoav},
  booktitle={Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  pages={5075--5084},
  year={2023}
}

@article{oren2024contamination,
  title={Proving Test Set Contamination in Black Box Language Models},
  author={Oren, Yonatan and Meister, Nicole and Chatterji, Niladri and Ladhak, Faisal and Hashimoto, Tatsunori B},
  journal={arXiv preprint arXiv:2310.17623},
  year={2024}
}

@article{bowman2021benchmarks,
  title={What Will it Take to Fix Benchmarking in Natural Language Understanding?},
  author={Bowman, Samuel R and Dahl, George E},
  journal={arXiv preprint arXiv:2104.02145},
  year={2021}
}

@article{madaan2023selfrefine,
  title={Self-Refine: Iterative Refinement with Self-Feedback},
  author={Madaan, Aman and Tandon, Niket and Gupta, Prakhar and Hallinan, Skyler and Gao, Luyu and Wiegreffe, Sarah and Alon, Uri and Dziri, Nouha and Prabhumoye, Shrimai and Yang, Yiming and others},
  journal={Advances in Neural Information Processing Systems},
  year={2023}
}

@article{shinn2023reflexion,
  title={Reflexion: Language Agents with Verbal Reinforcement Learning},
  author={Shinn, Noah and Cassano, Federico and Labash, Ashwin and Gopinath, Brahma and Narasimhan, Karthik and Yao, Shunyu},
  journal={Advances in Neural Information Processing Systems},
  year={2023}
}

@article{gulwani2017program,
  title={Program Synthesis},
  author={Gulwani, Sumit and Polozov, Oleksandr and Singh, Rishabh and others},
  journal={Foundations and Trends in Programming Languages},
  volume={4},
  number={1-2},
  pages={1--119},
  year={2017}
}

@article{ellis2021dreamcoder,
  title={{DreamCoder}: Bootstrapping Inductive Program Synthesis with Wake-Sleep Library Learning},
  author={Ellis, Kevin and Wong, Catherine and Nye, Maxwell and Sable-Meyer, Mathias and Morales, Luc and Hewitt, Luke and Cary, Lukas and Solar-Lezama, Armando and Tenenbaum, Joshua B},
  journal={Proceedings of the 42nd ACM SIGPLAN International Conference on Programming Language Design and Implementation},
  pages={835--850},
  year={2021}
}

@article{chollet2019measure,
  title={On the Measure of Intelligence},
  author={Chollet, Fran{\c{c}}ois},
  journal={arXiv preprint arXiv:1911.01547},
  year={2019}
}

@article{merrill2023expresssive,
  title={The Expressive Power of Transformers with Chain of Thought},
  author={Merrill, William and Sabharwal, Ashish},
  journal={arXiv preprint arXiv:2310.07923},
  year={2023}
}

@article{finn2017maml,
  title={Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks},
  author={Finn, Chelsea and Abbeel, Pieter and Levine, Sergey},
  journal={International Conference on Machine Learning},
  pages={1126--1135},
  year={2017}
}

@article{zhou2023leakage,
  title={Don't Make Your {LLM} an Evaluation Benchmark Cheater},
  author={Zhou, Kun and Zhu, Yutao and Chen, Zhipeng and Chen, Wentong and Zhao, Wayne Xin and Chen, Xu and Lin, Yankai and Wen, Ji-Rong and Han, Jiawei},
  journal={arXiv preprint arXiv:2311.01964},
  year={2023}
}

@article{xu2024leakage,
  title={Benchmark Data Contamination of Large Language Models: A Survey},
  author={Xu, Cheng and Guan, Shuhao and Greene, Derek and Kechadi, M-Tahar},
  journal={arXiv preprint arXiv:2406.04244},
  year={2024}
}

@article{deng2024memorization,
  title={Investigating Data Contamination in Modern Benchmarks for Large Language Models},
  author={Deng, Chunyuan and Zhao, Yilun and Tang, Xiangru and Gerstein, Mark and Cohan, Arman},
  journal={arXiv preprint arXiv:2311.09783},
  year={2024}
}

@article{raji2021benchmarks,
  title={{AI} and the Everything in the Whole Wide World Benchmark},
  author={Raji, Inioluwa Deborah and Bender, Emily M and Paullada, Amandalynne and Denton, Emily and Hanna, Alex},
  journal={Advances in Neural Information Processing Systems},
  year={2021}
}

@article{ribeiro2020behavioral,
  title={Beyond Accuracy: Behavioral Testing of {NLP} Models with {CheckList}},
  author={Ribeiro, Marco Tulio and Wu, Tongshuang and Guestrin, Carlos and Singh, Sameer},
  journal={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  pages={4902--4912},
  year={2020}
}

@article{ganin2016domain,
  title={Domain-Adversarial Training of Neural Networks},
  author={Ganin, Yaroslav and Ustinova, Evgeniya and Ajakan, Hana and Germain, Pascal and Larochelle, Hugo and Laviolette, Fran{\c{c}}ois and Marchand, Mario and Lempitsky, Victor},
  journal={The Journal of Machine Learning Research},
  volume={17},
  number={1},
  pages={2096--2030},
  year={2016}
}

@article{anil2022limits,
  title={Exploring Length Generalization in Large Language Models},
  author={Anil, Cem and Wu, Yuhuai and Andreassen, Anders and Lewkowycz, Aitor and Misra, Vedant and Ramasesh, Vinay and Slone, Ambrose and Gur-Ari, Guy and Dyer, Ethan and Neyshabur, Behnam},
  journal={Advances in Neural Information Processing Systems},
  year={2022}
}

@article{mitchell2021abstraction,
  title={Abstraction and Analogy-Making in Artificial Intelligence},
  author={Mitchell, Melanie},
  journal={arXiv preprint arXiv:2102.10717},
  year={2021}
}

@article{nye2021scratchpad,
  title={Show Your Work: Scratchpads for Intermediate Computation with Language Models},
  author={Nye, Maxwell and Andreassen, Anders Johan and Gur-Ari, Guy and Michalewski, Henryk and Austin, Jacob and Biber, David and Dohan, David and Lewkowycz, Aitor and Bosma, Maarten and Luan, David and others},
  journal={arXiv preprint arXiv:2112.00114},
  year={2021}
}

@article{li2023starcoder,
  title={{StarCoder}: May the Source Be with You!},
  author={Li, Raymond and Allal, Loubna Ben and Zi, Yangtian and Muennighoff, Niklas and Kocetkov, Denis and Mou, Chenghao and Marone, Marc and Akiki, Christopher and Li, Jia and Chim, Jenny and others},
  journal={arXiv preprint arXiv:2305.06161},
  year={2023}
}

@article{roziere2023codellama,
  title={Code {Llama}: Open Foundation Models for Code},
  author={Rozi{\`e}re, Baptiste and Gehring, Jonas and Gloeckle, Fabian and Sootla, Sten and Gat, Itai and Tan, Xiaoqing Ellen and Adi, Yossi and Liu, Jingyu and Remez, Tal and Rapin, J{\'e}r{\'e}my and others},
  journal={arXiv preprint arXiv:2308.12950},
  year={2023}
}

@article{nijkamp2023codegen,
  title={{CodeGen}: An Open Large Language Model for Code with Multi-Turn Program Synthesis},
  author={Nijkamp, Erik and Pang, Bo and Hayashi, Hiroaki and Tu, Lifu and Wang, Huan and Zhou, Yingbo and Savarese, Silvio and Xiong, Caiming},
  journal={International Conference on Learning Representations},
  year={2023}
}

@article{fried2023incoder,
  title={{InCoder}: A Generative Model for Code Infilling and Synthesis},
  author={Fried, Daniel and Aghajanyan, Armen and Lin, Jessy and Wang, Sida and Wallace, Eric and Shi, Freda and Zhong, Ruiqi and Yih, Wen-tau and Zettlemoyer, Luke and Lewis, Mike},
  journal={International Conference on Learning Representations},
  year={2023}
}

@article{wang2021codet5,
  title={{CodeT5}: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation},
  author={Wang, Yue and Wang, Weishi and Joty, Shafiq and Hoi, Steven CH},
  journal={Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
  pages={8696--8708},
  year={2021}
}

@article{lai2023ds1000,
  title={{DS-1000}: A Natural and Reliable Benchmark for Data Science Code Generation},
  author={Lai, Yuhang and Li, Chengxi and Wang, Yiming and Zhang, Tianyi and Zhong, Ruiqi and Zettlemoyer, Luke and Yih, Wen-tau and Fried, Daniel and Wang, Sida and Yu, Tao},
  journal={International Conference on Machine Learning},
  pages={18319--18345},
  year={2023}
}

@article{li2022alphacode,
  title={Competition-Level Code Generation with {AlphaCode}},
  author={Li, Yujia and Choi, David and Chung, Junyoung and Kushman, Nate and Schrittwieser, Julian and Leblond, R{\'e}mi and Eccles, Tom and Keeling, James and Gimeno, Felix and Dal Lago, Agustin and others},
  journal={Science},
  volume={378},
  number={6624},
  pages={1092--1097},
  year={2022}
}

@inproceedings{perez2021crossfit,
  title={{CrossFit}: A Few-shot Learning Challenge for Cross-Task Generalization in {NLP}},
  author={Ye, Qinyuan and Lin, Bill Yuchen and Ren, Xiang},
  booktitle={Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
  pages={7163--7189},
  year={2021}
}

@article{wang2022adversarial,
  title={Adversarial {GLUE}: A Multi-Task Benchmark for Robustness Evaluation of Language Models},
  author={Wang, Boxin and Xu, Chejian and Wang, Shuohang and Gan, Zhe and Cheng, Yu and Gao, Jianfeng and Awadallah, Ahmed Hassan and Li, Bo},
  journal={Advances in Neural Information Processing Systems},
  year={2022}
}

@article{openai2023gpt4,
  title={{GPT-4} Technical Report},
  author={OpenAI},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@article{anthropic2024claude,
  title={The {Claude} 3 Model Family: A New Standard for Intelligence},
  author={Anthropic},
  journal={Technical Report},
  year={2024}
}

@article{gemini2024technical,
  title={{Gemini}: A Family of Highly Capable Multimodal Models},
  author={Gemini Team and Google},
  journal={arXiv preprint arXiv:2312.11805},
  year={2024}
}

@article{goodhart1984monetary,
  title={Problems of Monetary Management: The {UK} Experience},
  author={Goodhart, Charles AE},
  journal={Monetary Theory and Practice},
  pages={91--121},
  year={1984},
  publisher={Springer}
}

@article{strathern1997improving,
  title={`Improving Ratings': Audit in the {British} University System},
  author={Strathern, Marilyn},
  journal={European Review},
  volume={5},
  number={3},
  pages={305--321},
  year={1997}
}

@article{cobbe2021gsm8k,
  title={Training Verifiers to Solve Math Word Problems},
  author={Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and others},
  journal={arXiv preprint arXiv:2110.14168},
  year={2021}
}

@article{hendrycks2021mmlu,
  title={Measuring Massive Multitask Language Understanding},
  author={Hendrycks, Dan and Burns, Collin and Basart, Steven and Zou, Andy and Mazeika, Mantas and Song, Dawn and Steinhardt, Jacob},
  journal={International Conference on Learning Representations},
  year={2021}
}
