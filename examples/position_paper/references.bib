% Benchmark Contamination and Data Leakage

@article{chandak2025answer,
  title={Answer Matching Outperforms Multiple Choice for Language Model Evaluation},
  author={Chandak, Nikhil and Goel, Shashwat and Prabhu, Ameya and Hardt, Moritz and Geiping, Jonas},
  journal={arXiv preprint arXiv:2507.02856},
  year={2025},
  note={Demonstrates that multiple-choice benchmarks can be solved without seeing questions, achieving 41\% on MMLU-Pro and 51\% on MMMU-Pro without question/image}
}

@article{zhang2024careful,
  title={A Careful Examination of Large Language Model Performance on Grade School Arithmetic},
  author={Zhang, Hugh and Da, Jeff and Lee, Dean and Robinson, Vaughn and Wu, Catherine and Song, Will and Zhao, Tiffany and Raja, Pranav and Zhuang, Charlotte and Slack, Dylan and Lyu, Qin and Hendryx, Sean and Kaplan, Russell and Lunati, Michele and Yue, Summer},
  journal={arXiv preprint arXiv:2405.00332},
  year={2024}
}

@inproceedings{jacovi2023stop,
  title={Stop Uploading Test Data in Plain Text: Practical Strategies for Mitigating Data Contamination by Evaluation Benchmarks},
  author={Jacovi, Alon and Caciularu, Avi and Mamou, Jonathan and Goldberg, Yoav and Tsarfaty, Reut},
  booktitle={Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  year={2023}
}

@inproceedings{sainz2023nlp,
  title={{NLP} Evaluation in Trouble: On the Need to Measure {LLM} Data Contamination for Each Benchmark},
  author={Sainz, Oscar and Campos, Jon and Garcia-Ferrero, Iker and Etxaniz, Julen and de Lacalle, Oier Lopez and Agirre, Eneko},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2023},
  year={2023}
}

@inproceedings{deng2024investigating,
  title={Investigating Data Contamination in Modern Benchmarks for Large Language Models},
  author={Deng, Chunyuan and Zhao, Yilun and Tang, Xiangru and Gerber, Mark and Ding, Ying},
  booktitle={Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics},
  year={2024}
}

@article{xu2024benchmarking,
  title={Benchmarking Benchmark Leakage in Large Language Models},
  author={Xu, Ruijie and Wang, Zengzhi and Fan, Run-Ze and Liu, Pengfei},
  journal={arXiv preprint arXiv:2404.18824},
  year={2024}
}

@article{zhou2023dont,
  title={Don't Make Your {LLM} an Evaluation Benchmark Cheater},
  author={Zhou, Kun and Zhu, Yutao and Chen, Zhipeng and Chen, Wentong and Zhao, Wayne Xin and Chen, Xu and Lin, Yankai and Wen, Ji-Rong and Han, Jiawei},
  journal={arXiv preprint arXiv:2311.01964},
  year={2023}
}

@inproceedings{oren2024proving,
  title={Proving Test Set Contamination in Black-Box Language Models},
  author={Oren, Meir and Hassid, Michael and Adi, Yossi and Schwartz, Roy},
  booktitle={International Conference on Learning Representations},
  year={2024}
}

% Test-Time Training and Learning

@inproceedings{sun2020test,
  title={Test-Time Training with Self-Supervision for Generalization under Distribution Shift},
  author={Sun, Yu and Wang, Xiaolong and Liu, Zhuang and Miller, John and Efros, Alexei A and Hardt, Moritz},
  booktitle={International Conference on Machine Learning},
  year={2020}
}

@inproceedings{wang2021tent,
  title={{TENT}: Fully Test-Time Adaptation by Entropy Minimization},
  author={Wang, Dequan and Shelhamer, Evan and Liu, Shaoteng and Olshausen, Bruno and Darrell, Trevor},
  booktitle={International Conference on Learning Representations},
  year={2021}
}

@inproceedings{liu2021ttt,
  title={{TTT}++: When Does Self-Supervised Test-Time Training Fail or Thrive?},
  author={Liu, Yuejiang and Kothari, Parth and van Delft, Bastien and Bellot-Gurlet, Baptiste and Mordan, Taylor and Alahi, Alexandre},
  booktitle={Advances in Neural Information Processing Systems},
  year={2021}
}

@article{zuo2025ttrl,
  title={{TTRL}: Test-Time Reinforcement Learning},
  author={Zuo, Yuxin and Zhang, Kaiyan and Sheng, Li and Qu, Shang and Cui, Ganqu and Zhu, Xuekai and Li, Haozhan and Zhang, Yuchen and Long, Xinwei and Hua, Ermo and Qi, Biqing and Sun, Youbang and Ma, Zhiyuan and Yuan, Lifan and Ding, Ning and Zhou, Bowen},
  journal={arXiv preprint arXiv:2504.16084},
  year={2025}
}

@inproceedings{gandelsman2022test,
  title={Test-Time Training with Masked Autoencoders},
  author={Gandelsman, Yosef and Sun, Yu and Chen, Xinlei and Efros, Alexei A},
  booktitle={Advances in Neural Information Processing Systems},
  year={2022}
}

@article{sun2024learning,
  title={Learning to (Learn at Test Time): {RNN}s with Expressive Hidden States},
  author={Sun, Yu and Li, Xinhao and Dalal, Karan and Xu, Jiarui and Vikram, Arjun and Zhang, Genghan and Dubois, Yann and Chen, Xinlei and Wang, Xiaolong and Koyejo, Sanmi and Hashimoto, Tatsunori and Guestrin, Carlos},
  journal={arXiv preprint arXiv:2407.04620},
  year={2024}
}

@article{yuksekgonul2026discover,
  title={Learning to Discover at Test Time},
  author={Yuksekgonul, Mert and Koceja, Daniel and Li, Xinhao and Bianchi, Federico and McCaleb, Jed and Wang, Xiaolong and Kautz, Jan and Choi, Yejin and Zou, James and Guestrin, Carlos and Sun, Yu},
  journal={arXiv preprint arXiv:2601.16175},
  year={2026}
}

% Continual Learning

@article{kirkpatrick2017overcoming,
  title={Overcoming Catastrophic Forgetting in Neural Networks},
  author={Kirkpatrick, James and Pascanu, Razvan and Rabinowitz, Neil and Veness, Joel and Desjardins, Guillaume and Rusu, Andrei A and Milan, Kieran and Quan, John and Ramalho, Tiago and Grabska-Barwinska, Agnieszka and Hassabis, Demis and Clopath, Claudia and Kumaran, Dharshan and Hadsell, Raia},
  journal={Proceedings of the National Academy of Sciences},
  volume={114},
  number={13},
  pages={3521--3526},
  year={2017}
}

@inproceedings{scialom2022fine,
  title={Fine-tuned Language Models Are Continual Learners},
  author={Scialom, Thomas and Chakrabarty, Tuhin and Muresan, Smaranda},
  booktitle={Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
  year={2022}
}

@article{wu2024continual,
  title={Continual Learning for Large Language Models: A Survey},
  author={Wu, Tongtong and Luo, Linhao and Li, Yuan-Fang and Pan, Shirui and Vu, Thuy-Trang and Haffari, Gholamreza},
  journal={arXiv preprint arXiv:2402.01364},
  year={2024}
}

% LLM Evaluation and Benchmarks

@article{chen2021evaluating,
  title={Evaluating Large Language Models Trained on Code},
  author={Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde de Oliveira and Kaplan, Jared and Edwards, Harrison and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and others},
  journal={arXiv preprint arXiv:2107.03374},
  year={2021}
}

@article{austin2021program,
  title={Program Synthesis with Large Language Models},
  author={Austin, Jacob and Odena, Augustus and Nye, Maxwell and Bosma, Maarten and Michalewski, Henryk and Dohan, David and Jiang, Ellen and Cai, Carrie and Terry, Michael and Le, Quoc and Sutton, Charles},
  journal={arXiv preprint arXiv:2108.07732},
  year={2021}
}

@inproceedings{hendrycks2021measuring,
  title={Measuring Massive Multitask Language Understanding},
  author={Hendrycks, Dan and Burns, Collin and Basart, Steven and Zou, Andy and Mazeika, Mantas and Song, Dawn and Steinhardt, Jacob},
  booktitle={International Conference on Learning Representations},
  year={2021}
}

@article{srivastava2023beyond,
  title={Beyond the Imitation Game: Quantifying and Extrapolating the Capabilities of Language Models},
  author={Srivastava, Aarohi and others},
  journal={Transactions on Machine Learning Research},
  year={2023}
}

@article{liang2023holistic,
  title={Holistic Evaluation of Language Models},
  author={Liang, Percy and others},
  journal={Transactions on Machine Learning Research},
  year={2023}
}

@inproceedings{zheng2023judging,
  title={Judging {LLM}-as-a-Judge with {MT}-Bench and Chatbot Arena},
  author={Zheng, Lianmin and Chiang, Wei-Lin and Sheng, Ying and Zhuang, Siyuan and Wu, Zhanghao and Zhuang, Yonghao and Lin, Zi and Li, Zhuohan and Li, Dacheng and Xing, Eric and Zhang, Hao and Gonzalez, Joseph E and Stoica, Ion},
  booktitle={Advances in Neural Information Processing Systems},
  year={2023}
}

% Meta-Learning and In-Context Learning

@inproceedings{finn2017model,
  title={Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks},
  author={Finn, Chelsea and Abbeel, Pieter and Levine, Sergey},
  booktitle={International Conference on Machine Learning},
  year={2017}
}

@inproceedings{brown2020language,
  title={Language Models are Few-Shot Learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  booktitle={Advances in Neural Information Processing Systems},
  year={2020}
}

@inproceedings{min2022rethinking,
  title={Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?},
  author={Min, Sewon and Lyu, Xinxi and Holtzman, Ari and Arber, Mikel and Lewis, Mike and Hajishirzi, Hannaneh and Zettlemoyer, Luke},
  booktitle={Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
  year={2022}
}

@inproceedings{chan2022data,
  title={Data Distributional Properties Drive Emergent In-Context Learning in Transformers},
  author={Chan, Stephanie and Santoro, Adam and Lampinen, Andrew and Wang, Jane and Singh, Aaditya and Richemond, Pierre and McClelland, James and Hill, Felix},
  booktitle={Advances in Neural Information Processing Systems},
  year={2022}
}

@article{wei2022emergent,
  title={Emergent Abilities of Large Language Models},
  author={Wei, Jason and Tay, Yi and Bommasani, Rishi and Raffel, Colin and Zoph, Barret and Borgeaud, Sebastian and Yogatama, Dani and Bosma, Maarten and Zhou, Denny and Metzler, Donald and Chi, Ed H and Hashimoto, Tatsunori and Vinyals, Oriol and Liang, Percy and Dean, Jeff and Fedus, William},
  journal={Transactions on Machine Learning Research},
  year={2022}
}

@article{olsson2022context,
  title={In-context Learning and Induction Heads},
  author={Olsson, Catherine and Elhage, Nelson and Nanda, Neel and Joseph, Nicholas and DasSarma, Nova and Henighan, Tom and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and others},
  journal={Transformer Circuits Thread},
  year={2022}
}

@inproceedings{xie2022explanation,
  title={An Explanation of In-Context Learning as Implicit {B}ayesian Inference},
  author={Xie, Sang Michael and Raghunathan, Aditi and Liang, Percy and Ma, Tengyu},
  booktitle={International Conference on Learning Representations},
  year={2022}
}

% Parameter-Efficient Fine-Tuning

@inproceedings{hu2022lora,
  title={{LoRA}: Low-Rank Adaptation of Large Language Models},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  booktitle={International Conference on Learning Representations},
  year={2022}
}

@inproceedings{dettmers2023qlora,
  title={{QLoRA}: Efficient Finetuning of Quantized {LLM}s},
  author={Dettmers, Tim and Pagnoni, Artidoro and Holtzman, Ari and Zettlemoyer, Luke},
  booktitle={Advances in Neural Information Processing Systems},
  year={2023}
}

@inproceedings{houlsby2019parameter,
  title={Parameter-Efficient Transfer Learning for {NLP}},
  author={Houlsby, Neil and Giurgiu, Andrei and Jastrzebski, Stanislaw and Morrone, Bruna and de Laroussilhe, Quentin and Gesmundo, Andrea and Attariyan, Mona and Gelly, Sylvain},
  booktitle={International Conference on Machine Learning},
  year={2019}
}

@inproceedings{li2021prefix,
  title={Prefix-Tuning: Optimizing Continuous Prompts for Generation},
  author={Li, Xiang Lisa and Liang, Percy},
  booktitle={Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics},
  year={2021}
}

@inproceedings{lester2021power,
  title={The Power of Scale for Parameter-Efficient Prompt Tuning},
  author={Lester, Brian and Al-Rfou, Rami and Constant, Noah},
  booktitle={Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
  year={2021}
}

% Code Generation and Evaluation

@article{li2022competition,
  title={Competition-Level Code Generation with {AlphaCode}},
  author={Li, Yujia and Choi, David and Chung, Junyoung and Kushman, Nate and Schrittwieser, Julian and Leblond, Remi and Eccles, Tom and Keeling, James and Gimeno, Felix and Lago, Agustin Dal and others},
  journal={Science},
  volume={378},
  number={6624},
  pages={1092--1097},
  year={2022}
}

@inproceedings{liu2024code,
  title={Is Your Code Generated by {ChatGPT} Really Correct? Rigorous Evaluation of Large Language Models for Code Generation},
  author={Liu, Jiawei and Xia, Chunqiu Steven and Wang, Yuyao and Zhang, Lingming},
  booktitle={Advances in Neural Information Processing Systems},
  year={2023}
}

@inproceedings{liu2024lost,
  title={Lost in the Middle: How Language Models Use Long Contexts},
  author={Liu, Nelson F and Lin, Kevin and Hewitt, John and Paranjape, Ashwin and Bevilacqua, Michele and Petroni, Fabio and Liang, Percy},
  booktitle={Transactions of the Association for Computational Linguistics},
  volume={12},
  pages={157--173},
  year={2024}
}

@inproceedings{jimenez2024swebench,
  title={{SWE}-bench: Can Language Models Resolve Real-World {GitHub} Issues?},
  author={Jimenez, Carlos E and Yang, John and Wettig, Alexander and Yao, Shunyu and Pei, Kexin and Press, Ofir and Narasimhan, Karthik},
  booktitle={International Conference on Learning Representations},
  year={2024}
}

@inproceedings{lai2023ds1000,
  title={{DS}-1000: A Natural and Reliable Benchmark for Data Science Code Generation},
  author={Lai, Yuhang and Li, Chengxi and Wang, Yiming and Zhang, Tianyi and Zhong, Ruiqi and Zettlemoyer, Luke and Yih, Wen-tau and Fried, Daniel and Wang, Sida and Yu, Tao},
  booktitle={International Conference on Machine Learning},
  year={2023}
}

@article{cassano2023multipl,
  title={{MultiPL-E}: A Scalable and Polyglot Approach to Benchmarking Neural Code Generation},
  author={Cassano, Federico and Gouwar, John and Nguyen, Daniel and Nguyen, Sydney and Phipps-Costin, Luna and Pinckney, Donald and Yee, Ming-Ho and Zi, Yangtian and Anderson, Carolyn Jane and Feldman, Molly Q and Guha, Arjun and Greenberg, Michael and Jangda, Abhinav},
  journal={IEEE Transactions on Software Engineering},
  year={2023}
}

% Reasoning and Self-Improvement

@inproceedings{madaan2024self,
  title={Self-Refine: Iterative Refinement with Self-Feedback},
  author={Madaan, Aman and Tandon, Niket and Gupta, Prakhar and Hallinan, Skyler and Gao, Luyu and Wiegreffe, Sarah and Alon, Uri and Dziri, Nouha and Prabhumoye, Shrimai and Yang, Yiming and Gupta, Shashank and Majumder, Bodhisattwa Prasad and Hermann, Katherine and Welleck, Sean and Yazdanbakhsh, Amir and Clark, Peter},
  booktitle={Advances in Neural Information Processing Systems},
  year={2023}
}

@inproceedings{shinn2023reflexion,
  title={Reflexion: Language Agents with Verbal Reinforcement Learning},
  author={Shinn, Noah and Cassano, Federico and Gopinath, Ashwin and Narasimhan, Karthik and Yao, Shunyu},
  booktitle={Advances in Neural Information Processing Systems},
  year={2023}
}

@inproceedings{zelikman2022star,
  title={{STaR}: Bootstrapping Reasoning with Reasoning},
  author={Zelikman, Eric and Wu, Yuhuai and Mu, Jesse and Goodman, Noah},
  booktitle={Advances in Neural Information Processing Systems},
  year={2022}
}

@inproceedings{wei2022chain,
  title={Chain-of-Thought Prompting Elicits Reasoning in Large Language Models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Ichter, Brian and Xia, Fei and Chi, Ed and Le, Quoc and Zhou, Denny},
  booktitle={Advances in Neural Information Processing Systems},
  year={2022}
}

@article{cobbe2021training,
  title={Training Verifiers to Solve Math Word Problems},
  author={Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and Hesse, Christopher and Schulman, John},
  journal={arXiv preprint arXiv:2110.14168},
  year={2021}
}

@inproceedings{kojima2022large,
  title={Large Language Models are Zero-Shot Reasoners},
  author={Kojima, Takeshi and Gu, Shixiang Shane and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke},
  booktitle={Advances in Neural Information Processing Systems},
  year={2022}
}

% RLHF and Training

@inproceedings{ouyang2022training,
  title={Training Language Models to Follow Instructions with Human Feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and Schulman, John and Hilton, Jacob and Kelton, Fraser and Miller, Luke and Simens, Maddie and Askell, Amanda and Welinder, Peter and Christiano, Paul F and Leike, Jan and Lowe, Ryan},
  booktitle={Advances in Neural Information Processing Systems},
  year={2022}
}

% Code LLMs

@article{roziere2024code,
  title={Code {Llama}: Open Foundation Models for Code},
  author={Roziere, Baptiste and others},
  journal={arXiv preprint arXiv:2308.12950},
  year={2024}
}

@article{gunasekar2023textbooks,
  title={Textbooks Are All You Need},
  author={Gunasekar, Suriya and Zhang, Yi and Anber, Jyoti and Mendes, Rui and Del Giorno, Allie and Gopi, Sivakanth and Javadi, Mojan and Kauffmann, Phoebe and de Rosa, Gustavo and Saarikivi, Olli and others},
  journal={arXiv preprint},
  year={2023}
}

% OOD Generalization

@inproceedings{ye2021crossfit,
  title={{CrossFit}: A Few-shot Learning Challenge for Cross-Task Generalization in {NLP}},
  author={Ye, Qinyuan and Lin, Bill Yuchen and Ren, Xiang},
  booktitle={Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
  year={2021}
}

@article{yang2023robustness,
  title={On the Robustness of {ChatGPT}: An Adversarial and Out-of-distribution Perspective},
  author={Yang, Yue and Nushi, Besmira and Palangi, Hamid and Mirzasoleiman, Baharan},
  journal={arXiv preprint},
  year={2023}
}

@inproceedings{arora2024ask,
  title={Ask Me Anything: A Simple Strategy for Prompting Language Models},
  author={Arora, Simran and Narayan, Avanika and Chen, Mayee F and Orber, Laurel and Zhang, Tony and Re, Christopher},
  booktitle={International Conference on Learning Representations},
  year={2024}
}

% Transformer Theory

@inproceedings{merrill2024expressive,
  title={The Expressive Power of Transformers with Chain of Thought},
  author={Merrill, William and Sabharwal, Ashish},
  booktitle={International Conference on Learning Representations},
  year={2024}
}

@inproceedings{anil2022exploring,
  title={Exploring Length Generalization in Large Language Models},
  author={Anil, Cem and Wu, Yuhuai and Andreassen, Anders and Lewkowycz, Aitor and Misra, Vedant and Ramasesh, Vinay and Slone, Ambrose and Gur-Ari, Guy and Dyer, Ethan and Neyshabur, Behnam},
  booktitle={Advances in Neural Information Processing Systems},
  year={2022}
}

% Additional relevant work

@inproceedings{xu2022systematic,
  title={A Systematic Evaluation of Large Language Models of Code},
  author={Xu, Frank F and Alon, Uri and Neubig, Graham and Hellendoorn, Vincent J},
  booktitle={Proceedings of the 6th ACM SIGPLAN International Symposium on Machine Programming},
  year={2022}
}

@inproceedings{muennighoff2024octopack,
  title={{OctoPack}: Instruction Tuning Code Large Language Models},
  author={Muennighoff, Niklas and Liu, Qian and Zebaze, Armel and Zheng, Qinkai and Hui, Binyuan and Zhuo, Terry Yue and Singh, Swayam and Tang, Xiangru and von Werra, Leandro and Longpre, Shayne},
  booktitle={International Conference on Learning Representations},
  year={2024}
}

@inproceedings{yin2024natural,
  title={Natural Language to Code Translation with Execution},
  author={Yin, Pengcheng and Li, Hao and Xiao, Qinkai and Fang, Jingwen and Agarwal, Khushboo and Neubig, Graham and Raghothaman, Mukund},
  booktitle={Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
  year={2024}
}

@inproceedings{hsu2021learning,
  title={Learning to Program from Natural Language to Code},
  author={Hsu, Jeffrey and Khashabi, Daniel and Kembhavi, Aniruddha},
  booktitle={Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics},
  year={2021}
}

% Esoteric Programming Languages

@misc{brainfuck1993,
  title={Brainfuck},
  author={M{\"u}ller, Urban},
  year={1993},
  note={Esoteric programming language with only 8 commands operating on a memory tape}
}

@misc{befunge1993,
  title={Befunge-93},
  author={Pressey, Chris},
  year={1993},
  note={Two-dimensional stack-based programming language}
}

@misc{whitespace2003,
  title={Whitespace},
  author={Brady, Edwin and Hammond, Kevin},
  year={2003},
  note={Programming language using only whitespace characters}
}

@misc{unlambda1999,
  title={Unlambda},
  author={Madore, David},
  year={1999},
  note={Minimal functional programming language based on combinatory logic}
}

@misc{shakespeare2001,
  title={The Shakespeare Programming Language},
  author={{\AA}slund, Karl and Hasselstr{\"o}m, Jon},
  year={2001},
  note={Programming language designed to look like a Shakespeare play}
}

@misc{piet2002,
  title={Piet},
  author={Morgan-Mar, David},
  year={2002},
  note={Programming language where programs are abstract art images}
}

% Domain Adaptation and Transfer Learning

@inproceedings{ben2010theory,
  title={A Theory of Learning from Different Domains},
  author={Ben-David, Shai and Blitzer, John and Crammer, Koby and Kulesza, Alex and Pereira, Fernando and Vaughan, Jennifer Wortman},
  booktitle={Machine Learning},
  volume={79},
  pages={151--175},
  year={2010}
}

@inproceedings{ganin2016domain,
  title={Domain-Adversarial Training of Neural Networks},
  author={Ganin, Yaroslav and Ustinova, Evgeniya and Ajakan, Hana and Germain, Pascal and Larochelle, Hugo and Laviolette, Fran{\c{c}}ois and Marchand, Mario and Lempitsky, Victor},
  booktitle={Journal of Machine Learning Research},
  volume={17},
  pages={1--35},
  year={2016}
}

@inproceedings{wang2022generalizing,
  title={Generalizing to Unseen Domains: A Survey on Domain Generalization},
  author={Wang, Jindong and Lan, Cuiling and Liu, Chang and Ouyang, Yidong and Qin, Tao and Lu, Wang and Chen, Yiqiang and Zeng, Wenjun and Yu, Philip S},
  booktitle={IEEE Transactions on Knowledge and Data Engineering},
  year={2022}
}

% Program Synthesis and Reasoning

@inproceedings{nye2021show,
  title={Show Your Work: Scratchpads for Intermediate Computation with Language Models},
  author={Nye, Maxwell and Andreassen, Anders Johan and Gur-Ari, Guy and Michalewski, Henryk and Austin, Jacob and Biber, David and Dohan, David and Lewkowycz, Aitor and Bosma, Maarten and Luan, David and Sutton, Charles and Odena, Augustus},
  booktitle={arXiv preprint arXiv:2112.00114},
  year={2021}
}

@inproceedings{ellis2021dreamcoder,
  title={{DreamCoder}: Bootstrapping Inductive Program Synthesis with Wake-Sleep Library Learning},
  author={Ellis, Kevin and Wong, Catherine and Nye, Maxwell and Sable-Meyer, Mathias and Morales, Luc and Hewitt, Luke and Cary, Lukas and Solar-Lezama, Armando and Tenenbaum, Joshua B},
  booktitle={Proceedings of the 42nd ACM SIGPLAN International Conference on Programming Language Design and Implementation},
  year={2021}
}

@article{gulwani2017program,
  title={Program Synthesis},
  author={Gulwani, Sumit and Polozov, Oleksandr and Singh, Rishabh},
  journal={Foundations and Trends in Programming Languages},
  volume={4},
  number={1-2},
  pages={1--119},
  year={2017}
}

% Learning from Feedback

@inproceedings{yao2023react,
  title={{ReAct}: Synergizing Reasoning and Acting in Language Models},
  author={Yao, Shunyu and Zhao, Jeffrey and Yu, Dian and Du, Nan and Shafran, Izhak and Narasimhan, Karthik and Cao, Yuan},
  booktitle={International Conference on Learning Representations},
  year={2023}
}

@inproceedings{le2022coderl,
  title={{CodeRL}: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning},
  author={Le, Hung and Wang, Yue and Gotmare, Akhilesh Deepak and Savarese, Silvio and Hoi, Steven C H},
  booktitle={Advances in Neural Information Processing Systems},
  year={2022}
}

@inproceedings{chen2023teaching,
  title={Teaching Large Language Models to Self-Debug},
  author={Chen, Xinyun and Lin, Maxwell and Sch{\"a}rli, Nathanael and Zhou, Denny},
  booktitle={International Conference on Learning Representations},
  year={2024}
}

% Reasoning and Abstraction

@inproceedings{chollet2019measure,
  title={On the Measure of Intelligence},
  author={Chollet, Fran{\c{c}}ois},
  booktitle={arXiv preprint arXiv:1911.01547},
  year={2019}
}

@inproceedings{mitchell2021abstraction,
  title={Abstraction and Reasoning in AI Systems},
  author={Mitchell, Melanie},
  booktitle={arXiv preprint arXiv:2102.10717},
  year={2021}
}

@inproceedings{dziri2024faith,
  title={Faith and Fate: Limits of Transformers on Compositionality},
  author={Dziri, Nouha and Lu, Ximing and Sclar, Melanie and Li, Xiang Lorraine and Jiang, Liwei and Lin, Bill Yuchen and West, Peter and Bhagavatula, Chandra and Le Bras, Ronan and Hwang, Jena D and Welleck, Sean and Smith, Noah A and Choi, Yejin},
  booktitle={Advances in Neural Information Processing Systems},
  year={2024}
}

% Scaling and Efficiency

@article{kaplan2020scaling,
  title={Scaling Laws for Neural Language Models},
  author={Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  journal={arXiv preprint arXiv:2001.08361},
  year={2020}
}

@article{hoffmann2022training,
  title={Training Compute-Optimal Large Language Models},
  author={Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and de Las Casas, Diego and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and others},
  journal={arXiv preprint arXiv:2203.15556},
  year={2022}
}

% Evaluation and Measurement

@inproceedings{ribeiro2020beyond,
  title={Beyond Accuracy: Behavioral Testing of {NLP} Models with {CheckList}},
  author={Ribeiro, Marco Tulio and Wu, Tongshuang and Guestrin, Carlos and Singh, Sameer},
  booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  year={2020}
}

@inproceedings{bowman2021fix,
  title={What Will it Take to Fix Benchmarking in Natural Language Understanding?},
  author={Bowman, Samuel R and Dahl, George E},
  booktitle={Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics},
  year={2021}
}

@article{raji2021ai,
  title={{AI} and the Everything in the Whole Wide World Benchmark},
  author={Raji, Inioluwa Deborah and Bender, Emily M and Paullada, Amandalynne and Denton, Emily and Hanna, Alex},
  journal={arXiv preprint arXiv:2111.15366},
  year={2021}
}

% Additional Code and LLM Research

@inproceedings{nijkamp2023codegen,
  title={{CodeGen}: An Open Large Language Model for Code with Multi-Turn Program Synthesis},
  author={Nijkamp, Erik and Pang, Bo and Hayashi, Hiroaki and Tu, Lifu and Wang, Huan and Zhou, Yingbo and Savarese, Silvio and Xiong, Caiming},
  booktitle={International Conference on Learning Representations},
  year={2023}
}

@article{lozhkov2024starcoder,
  title={{StarCoder} 2 and The Stack v2: The Next Generation},
  author={Lozhkov, Anton and Li, Raymond and Allal, Loubna Ben and Cassano, Federico and Lamy-Poirier, Joel and Tazi, Nouamane and Tang, Ao and Pykhtar, Dmytro and Liu, Jiawei and Wei, Yuxiang and others},
  journal={arXiv preprint arXiv:2402.19173},
  year={2024}
}

@inproceedings{fried2023incoder,
  title={{InCoder}: A Generative Model for Code Infilling and Synthesis},
  author={Fried, Daniel and Aghajanyan, Armen and Lin, Jessy and Wang, Sida and Wallace, Eric and Shi, Freda and Zhong, Ruiqi and Yih, Wen-tau and Zettlemoyer, Luke and Lewis, Mike},
  booktitle={International Conference on Learning Representations},
  year={2023}
}

@inproceedings{wang2023codet5,
  title={{CodeT5}+: Open Code Large Language Models for Code Understanding and Generation},
  author={Wang, Yue and Le, Hung and Gotmare, Akhilesh Deepak and Bui, Nghi D Q and Li, Junnan and Hoi, Steven C H},
  booktitle={Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  year={2023}
}

% Efficient Test-Time Reasoning

@article{sharma2025think,
  title={Think Just Enough: Sequence-Level Entropy as a Confidence Signal for {LLM} Reasoning},
  author={Sharma, Aman and Chopra, Paras},
  journal={arXiv preprint arXiv:2510.08146},
  year={2025},
  note={Entropy-based early stopping for efficient reasoning, achieving 25-50\% computational savings}
}

@article{sharma2025sequential,
  title={The Sequential Edge: Inverse-Entropy Voting Beats Parallel Self-Consistency at Matched Compute},
  author={Sharma, Aman and Chopra, Paras},
  journal={arXiv preprint arXiv:2511.02309},
  year={2025},
  note={Sequential reasoning outperforms parallel self-consistency in 95.6\% of configurations}
}
